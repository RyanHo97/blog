## 网络爬虫系统学习

前言

课程名称：(黑马Java基础+就业IDEA版)-阶段四【7-网络爬虫课程】

课程地址：https://www.bilibili.com/video/BV1Yx411f7kF

笔记作者：https://ryanho97.github.io/

笔记时间：2021/03/10





## 正文

网络爬虫（Web crawler），是一种按照一定规则，自动地抓取万维网信息的程序或脚本。



### 1.爬虫入门程序



环境准备：

- JDK1.8
- Intellij IDEA
- Maven



开始创建：

1. 创建Maven工程

2. pom.xml：

   - Apache HttpClient

     ```xml
     <!-- https://mvnrepository.com/artifact/org.apache.httpcomponents/httpclient -->
     <dependency>
         <groupId>org.apache.httpcomponents</groupId>
         <artifactId>httpclient</artifactId>
         <version>4.5.2</version>
     </dependency>
     ```

   - SLF4J LOG4J 12

     ```xml
     <!-- https://mvnrepository.com/artifact/org.slf4j/slf4j-log4j12 -->
     <dependency>
         <groupId>org.slf4j</groupId>
         <artifactId>slf4j-log4j12</artifactId>
         <version>1.7.25</version>
         <scope>test</scope>
     </dependency>
     
     ```

3. 创建CrawlerFirst.java

   ```java
   package com.ryan.test;
   
   import org.apache.http.HttpEntity;
   import org.apache.http.client.methods.CloseableHttpResponse;
   import org.apache.http.client.methods.HttpGet;
   import org.apache.http.impl.client.CloseableHttpClient;
   import org.apache.http.impl.client.HttpClients;
   import org.apache.http.util.EntityUtils;
   
   import java.io.IOException;
   
   public class CrawlerFirst {
       public static void main(String[] args) throws IOException {
           //1.打开浏览器，创建HttpClient对象
           CloseableHttpClient httpClient = HttpClients.createDefault();
   
           //2.输入网址,发起get请求 
           HttpGet httpGet = new HttpGet("http://www.itcast.cn");
   
           //3.按回车，发起请求，返回响应，使用HttpClient对象发起请求
           CloseableHttpResponse response = httpClient.execute(httpGet);
   
           //4.解析响应，获取数据
           //判断状态码是否是200
           if(response.getStatusLine().getStatusCode() == 200){
               HttpEntity httpEntity = response.getEntity();
               String content = EntityUtils.toString(httpEntity, "utf8");
   
               System.out.println(content);
           }
   
       }
   }
   
   ```

4. 成功返回结果



### 2.网络爬虫介绍

大数据时代，信息的采集是一项重要的工作，而互联网中有海量的数据，人力采集低效繁琐，成本也高。

如何自动高效地获取互联网中的信息并为我们所用是一个重要的问题，而爬虫技术就是为了解决这些问题而生的。

网络爬虫（Web crawler）也叫做网络机器人，可以代替人们自动地在互联网中进行数据信息的采集与整理。它是一种按照一定的规则，自动的抓取万维网信息的程序或者脚本，可以自动采集所有能够访问到的页面内容，以获取相关数据。

功能上讲：数据采集，处理，储存

流程上讲：从一个或若干初始网页的URL开始，获得初始网页上的URL，在抓取网页的过程中，不断从当前页面上抽取新的URL放入队列，直到满足系统的一定停止条件。

### 3.为什么学网络爬虫

- 可以实现搜索引擎
- 大数据时代，可以让我们获取更多的数据源
- 可以更好地进行搜索引擎优化（SEO）
- 有利于就业



### 4.HttpClient

​	HTTP协议访问互联网的网页，网络爬虫需要编写程序，在这里同样使用地HTTP协议访问网页。

​	这里我们使用Java地HTTP协议客户端HttpClient这个技术，来实现抓取网页的数据。



- #### 4.1	GET请求

访问传智官网，请求url地址：

http://www.itcast.cn/



创建HttpGetTest.java

```java
package com.ryan.test;

import org.apache.http.client.HttpClient;
import org.apache.http.client.methods.CloseableHttpResponse;
import org.apache.http.client.methods.HttpGet;
import org.apache.http.impl.client.CloseableHttpClient;
import org.apache.http.impl.client.HttpClients;
import org.apache.http.util.EntityUtils;

import java.io.IOException;

public class HttpGetTest {
    public static void main(String[] args) {
        //创建HttpClient对象
        CloseableHttpClient httpClient = HttpClients.createDefault();

        //创建HttpGet对象，设置url访问地址
        HttpGet httpGet = new HttpGet("http://www.itcast.cn");

        CloseableHttpResponse response = null;
        try {
            //使用HttpClient发起请求，获取response
            response = httpClient.execute(httpGet);

            //解析响应
            if (response.getStatusLine().getStatusCode() == 200){
                String content = EntityUtils.toString(response.getEntity(), "utf8");
                System.out.println(content.length());
            }

        } catch (IOException e) {
            e.printStackTrace();
        } finally {
            //关闭response
            try {
                response.close();
            } catch (IOException e) {
                e.printStackTrace();
            }
            try {
                httpClient.close();
            } catch (IOException e) {
                e.printStackTrace();
            }

        }

        //解析响应

    }
}

```



pom中要注释slf4j-log4j12此行：

```xml
<!--<scope>test</scope>-->
```

这样就有日志产生。



- #### 4.2. 带参数的GET请求

在传智中搜索学习视频，地址为：

http://yun.itheima/search?keys=Java

创建HttpGetParamTest.java

```java
package com.ryan.test;

import org.apache.http.client.methods.CloseableHttpResponse;
import org.apache.http.client.methods.HttpGet;
import org.apache.http.client.utils.URIBuilder;
import org.apache.http.impl.client.CloseableHttpClient;
import org.apache.http.impl.client.HttpClients;
import org.apache.http.util.EntityUtils;

import java.io.IOException;

public class HttpGetParamTest {
    public static void main(String[] args) throws Exception {
        //创建HttpClient对象
        CloseableHttpClient httpClient = HttpClients.createDefault();

        //设置请求地址：http://yun.itheima/search?keys=Java
        //创建URIBuilder
        URIBuilder uriBuilder = new URIBuilder("http://yun.itheima.com/search");
        //设置参数
        uriBuilder.setParameter("keys","Java");
        //创建HttpGet对象，设置url访问地址
        HttpGet httpGet = new HttpGet(uriBuilder.build());

        System.out.println("发起请求的信息："+httpGet);

        CloseableHttpResponse response = null;
        try {
            //使用HttpClient发起请求，获取response
            response = httpClient.execute(httpGet);

            //解析响应
            if (response.getStatusLine().getStatusCode() == 200){
                String content = EntityUtils.toString(response.getEntity(), "utf8");
                System.out.println(content.length());
            }

        } catch (IOException e) {
            e.printStackTrace();
        } finally {
            //关闭response
            try {
                response.close();
            } catch (IOException e) {
                e.printStackTrace();
            }
            try {
                httpClient.close();
            } catch (IOException e) {
                e.printStackTrace();
            }

        }

        //解析响应

    }
}

```



- #### 4.3. POST请求

创建HttpPostTest.java

```java
package com.ryan.test;

import org.apache.http.client.methods.CloseableHttpResponse;
import org.apache.http.client.methods.HttpPost;
import org.apache.http.impl.client.CloseableHttpClient;
import org.apache.http.impl.client.HttpClients;
import org.apache.http.util.EntityUtils;

import java.io.IOException;

public class HttpPostTest {
    public static void main(String[] args) {
        //创建HttpClient对象
        CloseableHttpClient httpClient = HttpClients.createDefault();

        //创建HttpPost对象，设置url访问地址
        HttpPost httpPost = new HttpPost("http://www.itcast.cn");

        CloseableHttpResponse response = null;
        try {
            //使用HttpClient发起请求，获取response
            response = httpClient.execute(httpPost);

            //解析响应
            if (response.getStatusLine().getStatusCode() == 200){
                String content = EntityUtils.toString(response.getEntity(), "utf8");
                System.out.println(content.length());
            }

        } catch (IOException e) {
            e.printStackTrace();
        } finally {
            //关闭response
            try {
                response.close();
            } catch (IOException e) {
                e.printStackTrace();
            }
            try {
                httpClient.close();
            } catch (IOException e) {
                e.printStackTrace();
            }

        }

        //解析响应

    }
}

```



- #### 4.4. 带参数的POST请求

在传智中搜索学习视频，使用POST请求，url地址为：

http://yun.itheima/search?keys=Java

url地址没有参数，参数keys=java放到表单中进行提交



NameValuePair

```java
//
// Source code recreated from a .class file by IntelliJ IDEA
// (powered by Fernflower decompiler)
//

package org.apache.http;

public interface NameValuePair {
    String getName();//获取参数的名字

    String getValue();//获取参数的值
}

```



- #### 4.5. 连接池

  如果每次请求都要创建HttpClient，会有频繁创建和销毁的问题，可以使用连接池来解决这个问题。

  ```java
  PoolingHttpClientConnectionManager
  ```



​		创建HttpGetParamTest.java

```java
package com.ryan.test;

import org.apache.http.client.methods.CloseableHttpResponse;
import org.apache.http.client.methods.HttpGet;
import org.apache.http.impl.client.CloseableHttpClient;
import org.apache.http.impl.client.HttpClients;
import org.apache.http.impl.conn.PoolingHttpClientConnectionManager;
import org.apache.http.util.EntityUtils;


import java.io.IOException;

public class HttpClientPoolTest {

    public static void main(String[] args) {
        //创建连接池管理器
        PoolingHttpClientConnectionManager cm = new PoolingHttpClientConnectionManager();

        //设置连接数
        cm.setMaxTotal(100);

        //设置每个主机的最大连接数
        cm.setDefaultMaxPerRoute(10);

        //使用连接池管理器发起请求
        doGet(cm);
        doGet(cm);

    }

    private static void doGet(PoolingHttpClientConnectionManager cm){
        //不是每次创建新的HttpClient，而是从连接池中获取HttpClient对象
        CloseableHttpClient httpClient = HttpClients.custom().setConnectionManager(cm).build();

        HttpGet httpGet = new HttpGet("http://www.itcast.cn");

        CloseableHttpResponse response = null;

        try {
            response = httpClient.execute(httpGet);

            if(response.getStatusLine().getStatusCode() == 200){
                String content = EntityUtils.toString(response.getEntity(), "utf8");

                System.out.println(content.length());
            }

        } catch (IOException e) {
            e.printStackTrace();
        } finally {
            if (response !=null ){
                try {
                    response.close();
                } catch (IOException e) {
                    e.printStackTrace();
                }
                //不能关闭HttpClient，由连接池管理HttpClient
                //httpClient.close();
            }
        }

    }
}

```



#### 4.6. 请求参数

​		有时候因为网络，或者目标服务器的原因，请求需要更长的时间才能完成，需要自定义相关时间






### 5.Jsoup

我们抓取到页面之后，还需要对页面进行解析。可以使用字符串处理工具解析页面，也可以使用正则表达式，但是这些方法都会带来很大的开发成本，所以我们需要使用一款专门解析html页面的技术。

#### 5.1.jsoup介绍

jsoup是一款Java的HTML解析器，可直接解析某个URL地址、HTML文本内容。它提供了一套非常省力的API，可通过DOM，CSS以及类似于jQuery的操作方法来取出和操作数据。



jsoup的主要功能如下：

- 从一个URL，文字或字符串中解析HTML；
- 使用DOM或CSS选择器来查找、取出数据；
- 可操作HTML元素、属性、文本；



先加入Jsoup、junit、commons-io、commons-lang3依赖：

```xml
<!-- Jsoup解析网页 -->
<dependency>
	<groupId>org.jsoup</groupId>
	<artifactId>jsoup</artifactId>
	<version>1.9.2</version>
</dependency>
```

```xml
<!-- https://mvnrepository.com/artifact/junit/junit -->
<dependency>
    <groupId>junit</groupId>
    <artifactId>junit</artifactId>
    <version>4.12</version>
    <scope>test</scope>
</dependency>
```

```xml
<!-- https://mvnrepository.com/artifact/commons-io/commons-io -->
<dependency>
    <groupId>commons-io</groupId>
    <artifactId>commons-io</artifactId>
    <version>2.6</version>
</dependency>
```

```xml
<!-- https://mvnrepository.com/artifact/org.apache.commons/commons-lang3 -->
<dependency>
    <groupId>org.apache.commons</groupId>
    <artifactId>commons-lang3</artifactId>
    <version>3.7</version>
</dependency>
```





#### 5.2.1.Jsoup解析Url

代码：

```java
package jsoup;

import org.jsoup.Jsoup;
import org.jsoup.nodes.Document;
import org.junit.Test;

import java.net.URL;

public class JsoupFirstTest {

    @Test
    public void testUrl()throws Exception{
        //解析url地址,第一个参数是访问的url，第二个参数是访问时候的超时时间
        Document doc = Jsoup.parse(new URL("https://www.cgf.org.cn/n.html?id=7f199794-a9eb-4e1a-a854-f671565e8ee8&URLparamName=%E9%87%8D%E5%A4%A7%E6%B4%BB%E5%8A%A8"), 20000);

        //使用标签选择器，获取title标签中的内容
        String title = doc.getElementsByTag("title").first().text();

        //打印
        System.out.println(title);
    }
}

```

结果：

```text
中国绿化基金会官方网站

Process finished with exit code 0
```

PS：虽然使用Jsoup可以代替HttpClient直接发起请求解析数据，但是往往不会这样用，因为实际的开发过程中，需要使用到多线程，连接池，代理等等方式，而jsoup对这些的支持并不是很好，所以我们一般把jsoup仅仅作为Html解析工具使用。



#### 5.2.2.Jsoup解析字符串

1.准备一个html

2.java代码

```java
    @Test
    public void testString() throws Exception{
        //使用工具类读取文件，获取字符串
        String content = FileUtils.readFileToString(new File("html文件路径替换"), "utf8");

        //解析字符串
        Document doc = Jsoup.parse(content);

        String title = doc.getElementsByTag("title").first().text();

        System.out.println(title);

    }
```



#### 5.2.3.Jsoup解析文件

代码

```java
    @Test
    public void testFile()throws Exception{
        //解析文件
        Document doc = Jsoup.parse(new File("html文件路径替换"), "utf8");

        String title = doc.getElementsByTag("title").first().text();

        System.out.println(title);
    }
```


#### 5.2.4.使用Dom的方式获取元素

代码

```java
    @Test
    public void testDOM()throws Exception{
        //解析url地址,第一个参数是访问的url，第二个参数是访问时候的超时时间
        Document doc = Jsoup.parse(new URL("https://www.cgf.org.cn/n.html?id=7f199794-a9eb-4e1a-a854-f671565e8ee8&URLparamName=%E9%87%8D%E5%A4%A7%E6%B4%BB%E5%8A%A8"), 20000);

        //获取元素
        //1.根据id查询元素getElementById
        Element element = doc.getElementById("newList");
        //2.根据标签获取元素getElementByTag
        Element element1 = doc.getElementsByTag("h3").first();
        //3.根据class获取元素getElementByClass
        Element element2 = doc.getElementsByClass("detailBar").first();
        //4.根据属性获取元素getElementsByAttribute
        Element element3 = doc.getElementsByAttribute("abc").first();
        Element element4 = doc.getElementsByAttributeValue("abc","123").first();
        //打印元素的内容
        System.out.println(element2.text());
    }
```

